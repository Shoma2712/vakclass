{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -r requirements.txt",
   "id": "75dba26894df1481",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# БЛОК 1\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import pymorphy3\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import warnings\n",
    "import logging\n",
    "import ast\n",
    "\n",
    "# Отображение графиков в ноутбуке\n",
    "%matplotlib inline\n",
    "\n",
    "# Отключение предупреждений\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*datetime.utcnow.*\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::DeprecationWarning\"\n",
    "warnings.filterwarnings(\"ignore\", message=\".*multi-threaded, use of fork().*\")\n",
    "\n",
    "# Отключаем логирование для модуля pdfminer\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "# Загрузка ресурсов NLTK\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Настройка локальных путей\n",
    "# Получаем текущую директорию\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Создаем папку project_data рядом\n",
    "BASE_DIR = os.path.join(current_dir, \"project_data\")\n",
    "PDF_FOLDER = os.path.join(BASE_DIR, \"pdfs\")\n",
    "OUTPUT_CSV = os.path.join(BASE_DIR, \"dataset_final.csv\")\n",
    "TOPIC_CSV = os.path.join(BASE_DIR, \"dataset_with_topics.csv\")\n",
    "\n",
    "# Создаем папки физически\n",
    "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"Рабочая папка проекта: {BASE_DIR}\")\n",
    "print(f\"Папка для PDF: {PDF_FOLDER}\")"
   ],
   "id": "7deab7dd6d19e12a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# БЛОК 2 Базовые настройки\n",
    "base_url = \"https://imt-journal.ru\"\n",
    "archive_url = f\"{base_url}/archive\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def download_pdf(pdf_url, folder):\n",
    "    filename = os.path.join(folder, pdf_url.split(\"/\")[-1])\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Файл уже есть: {filename}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        print(f\"Скачивание: {pdf_url.split('/')[-1]}\")\n",
    "        response = requests.get(pdf_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка скачивания {pdf_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_pdfs_from_page(page_url):\n",
    "    pdf_urls = []\n",
    "    try:\n",
    "        response = requests.get(page_url, headers=headers, timeout=30)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            if a['href'].endswith(\".pdf\"):\n",
    "                full_url = urljoin(base_url, a['href'])\n",
    "                if full_url not in pdf_urls:\n",
    "                    pdf_urls.append(full_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка страницы {page_url}: {e}\")\n",
    "    return pdf_urls\n",
    "\n",
    "def run_scraper():\n",
    "    print(\"ЗАПУСК СБОРА PDF\")\n",
    "    visited_pages = set()\n",
    "    current_page = archive_url\n",
    "    page_num = 1\n",
    "    max_pages = 50\n",
    "\n",
    "    # Скачиваем PDF с текущей страницы\n",
    "    while current_page and current_page not in visited_pages and page_num <= max_pages:\n",
    "        print(f\"\\nСтраница {page_num}: {current_page}\")\n",
    "        visited_pages.add(current_page)\n",
    "\n",
    "        pdfs = get_pdfs_from_page(current_page)\n",
    "        print(f\"Найдено PDF: {len(pdfs)}\")\n",
    "\n",
    "        for pdf_url in pdfs:\n",
    "            download_pdf(pdf_url, PDF_FOLDER)\n",
    "            time.sleep(0.5) #Пауза, чтобы не дудосить сайт\n",
    "\n",
    "        #Ищем следующую страницу\n",
    "        try:\n",
    "            response = requests.get(current_page, headers=headers, timeout=30)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            next_link = None\n",
    "            pagination = soup.find(class_='pagination')\n",
    "            if pagination:\n",
    "                curr = pagination.find('span', class_='current')\n",
    "                if curr and curr.find_next_sibling('a'):\n",
    "                    next_link = curr.find_next_sibling('a')['href']\n",
    "\n",
    "            if not next_link:\n",
    "                 for a in soup.find_all('a', href=True):\n",
    "                    if a.get_text(strip=True) in ['>', '»', 'Вперед', 'Next', 'Следующая']:\n",
    "                        next_link = a['href']\n",
    "                        break\n",
    "\n",
    "            if next_link:\n",
    "                next_page_url = urljoin(current_page, next_link)\n",
    "                if next_page_url == current_page or next_page_url in visited_pages:\n",
    "                    break\n",
    "                current_page = next_page_url\n",
    "                page_num += 1\n",
    "            else:\n",
    "                print(\"Последняя страница.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка пагинации: {e}\")\n",
    "            break\n",
    "\n",
    "# Запускаем скрейпер\n",
    "run_scraper()"
   ],
   "id": "5720fdc60f2e81ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# БЛОК 3 Парсинг, обработка текста и создание датаета\n",
    "import tqdm\n",
    "\n",
    "print(\"НАЧАЛО ОБРАБОТКИ PDF И СОЗДАНИЯ ДАТАСЕТА\")\n",
    "\n",
    "# Инициализация морфологического анализатора и стоп-слов\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Расширенные стоп-слова\n",
    "academic_stopwords = [ 'это', 'наш', 'год', 'рис', 'табл', 'стр', 'см', 'статья', 'работа', 'исследование', 'данные', 'результат', 'метод', 'основа', 'использование', 'показать', 'рассмотреть', 'являться', 'предложить', 'провести', 'задача', 'рисунок', 'таблица', 'схема', 'цель', 'проблема', 'решение', 'пример', 'случай', 'вид', 'тип', 'сравнение', 'оценка', 'вопрос', 'подход', 'номер', 'онтология', 'применение', 'область', 'научный', 'новый', 'современный', 'различный', 'получить', 'выполнить', 'представить', 'описать', 'существовать', 'позволять', 'введение', 'заключение', 'вывод', 'список', 'литература', 'библиография', 'аннотация', 'ключевые', 'слова', 'удк', 'doi', 'том', 'выпуск', 'автор', 'данный', 'использовать', 'свойство', 'разработка', 'определение', 'параметр', 'ключевой', 'который', 'свой', 'этот', 'также', 'каждый', 'мочь', 'весь']\n",
    "stop_words.update(academic_stopwords)\n",
    "\n",
    "def tokenize_for_lda(text):\n",
    "    if not text: return []\n",
    "\n",
    "    # Склеивание переносов слов\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Удаление всего кроме букв\n",
    "    text = re.sub(r'[^а-яёА-ЯЁ\\s]', ' ', text)\n",
    "\n",
    "    # Приведение к нижнему регистру и удаление лишних пробелов\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if len(word) <= 2: continue\n",
    "\n",
    "        # Морфологический анализ\n",
    "        p = morph.parse(word)[0]\n",
    "\n",
    "        # Фильтрация по частям речи для LDA (существительные и прилагательные)\n",
    "        if p.tag.POS not in {'NOUN', 'ADJF'}: continue\n",
    "\n",
    "        lemma = p.normal_form\n",
    "        if lemma not in stop_words:\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "# Извлекает текст из PDF с обрезкой колонтитулов\n",
    "def extract_text_with_crop(pdf_path):\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                width, height = page.width, page.height\n",
    "                cropped_page = page.crop((0, 60, width, height - 60))\n",
    "                page_text = cropped_page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка чтения {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "    return full_text\n",
    "\n",
    "# Разбивает текст на статьи по УДК и выделяет смысловую часть\n",
    "def parse_articles_from_text(raw_text, filename):\n",
    "    articles_data = []\n",
    "\n",
    "    # Разбиение полного текста файла на куски по маркеру УДК\n",
    "    chunks = re.split(r'\\n(?=УДК\\s+[\\d\\.\\+\\:]+)', raw_text)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Пропускаем слишком короткие фрагменты\n",
    "        if len(chunk) < 1000: continue\n",
    "\n",
    "        udc_match = re.search(r'УДК\\s+([\\d\\.\\+\\:]+)', chunk)\n",
    "        udc = udc_match.group(1) if udc_match else \"N/A\"\n",
    "\n",
    "        # Извлечение аннотации\n",
    "        annot_match = re.search(r'Аннотация\\.?(.*?)(Ключевые слова|Keywords)', chunk, re.DOTALL | re.IGNORECASE)\n",
    "        annotation = annot_match.group(1).strip() if annot_match else \"\"\n",
    "\n",
    "        # Извлечение ключевых слов\n",
    "        kw_match = re.search(r'Ключевые слова[:\\.](.*?)(Цитирование|Citation|Введение|Introduction)', chunk, re.DOTALL | re.IGNORECASE)\n",
    "        keywords = kw_match.group(1).strip() if kw_match else \"\"\n",
    "\n",
    "        # Поиск начала основного текста\n",
    "        intro_match = re.search(r'(?:\\n|^)\\s*(?:1\\.?)?\\s*(?:Введение|Introduction)(?:\\.|:)?\\s+', chunk, re.IGNORECASE)\n",
    "        main_text_raw = \"\"\n",
    "\n",
    "        if intro_match:\n",
    "            start_pos = intro_match.end()\n",
    "            main_text_raw = chunk[start_pos:]\n",
    "        else:\n",
    "            fallback_match = re.search(r'(Ключевые слова|Keywords|Цитирование|Citation).*?(\\n\\s*\\n|\\n[А-Я])', chunk, re.DOTALL | re.IGNORECASE)\n",
    "            if fallback_match:\n",
    "                start_pos = fallback_match.end()\n",
    "                main_text_raw = chunk[start_pos:]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Поиск конца текста (обрезаем текст до списка литературы)\n",
    "        ref_start = re.search(r'(?:\\n|^)\\s*(?:СПИСОК ЛИТЕРАТУРЫ|Список источников|REFERENCES|Библиографический список)', main_text_raw, re.IGNORECASE)\n",
    "        if ref_start:\n",
    "            main_text_raw = main_text_raw[:ref_start.start()]\n",
    "\n",
    "        # Токенизация и проверка на длину\n",
    "        lda_tokens = tokenize_for_lda(main_text_raw)\n",
    "\n",
    "        if len(lda_tokens) > 50:\n",
    "            articles_data.append({ 'source': filename, 'udc': udc, 'annotation': annotation, 'keywords': keywords, 'token_len': len(lda_tokens),\n",
    "                                   'main_text_sample': main_text_raw[:200].replace('\\n', ' ') + \"...\", 'lda_tokens': lda_tokens})\n",
    "    return articles_data\n",
    "\n",
    "# Запуск\n",
    "all_articles = []\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    print(\"Найден существующий CSV, читаем его\")\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "    # Конвертация строки списка обратно в список\n",
    "    df['lda_tokens'] = df['lda_tokens'].apply(ast.literal_eval)\n",
    "else:\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.endswith(\".pdf\")]\n",
    "    if not pdf_files:\n",
    "        print(\"PDF файлы не найдены в папке:\", PDF_FOLDER)\n",
    "        df = pd.DataFrame() # Пустой датафрейм чтобы не упало дальше\n",
    "    else:\n",
    "        for pdf_file in tqdm.tqdm(pdf_files, desc=\"Парсинг PDF\"):\n",
    "            full_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "            raw_pdf_text = extract_text_with_crop(full_path)\n",
    "            if raw_pdf_text:\n",
    "                articles = parse_articles_from_text(raw_pdf_text, pdf_file)\n",
    "                all_articles.extend(articles)\n",
    "\n",
    "        # Создание DataFrame\n",
    "        df = pd.DataFrame(all_articles)\n",
    "        if not df.empty:\n",
    "            cols = ['source', 'udc', 'token_len', 'annotation', 'keywords', 'main_text_sample', 'lda_tokens']\n",
    "            df = df[cols]\n",
    "            df.to_csv(OUTPUT_CSV, index=False)\n",
    "            print(f\"Готово! Статей в базе: {len(df)}\")\n",
    "            print(f\"Пример данных (первые 5 строк):\")\n",
    "            print(df[['main_text_sample']].head())\n",
    "        else:\n",
    "            print(\"Валидные статьи не сформированы.\")"
   ],
   "id": "7387dcd08d791c2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# БЛОК 4\n",
    "if df is not None and not df.empty:\n",
    "    print(f\"Всего статей: {len(df)}\")\n",
    "\n",
    "    # Сбор всех слов\n",
    "    all_words = []\n",
    "    for tokens in df['lda_tokens']:\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = ast.literal_eval(tokens)\n",
    "        all_words.extend(tokens)\n",
    "\n",
    "    # Гистограмма Топ-20\n",
    "    word_freq = Counter(all_words)\n",
    "    common_words = word_freq.most_common(20)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar([x[0] for x in common_words], [x[1] for x in common_words], color='steelblue')\n",
    "    plt.title(\"Топ-20 слов во всем корпусе\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Глобальное облако слов\n",
    "    print(\"Генерация глобального облака слов\")\n",
    "    text_cloud = \" \".join(all_words)\n",
    "    wordcloud = WordCloud(width=1600, height=800, background_color='white', colormap='viridis').generate(text_cloud)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Облако слов (Весь датасет)\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Нет данных.\")"
   ],
   "id": "54e3acab0321be06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# БЛОК 5 Тематическое моделирование LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel, LdaMulticore\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Отключаем логи Gensim\n",
    "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(f\"Всего статей для анализа: {len(df)}\")\n",
    "\n",
    "    # 1. Подготовка данных\n",
    "    data_words = list(df['lda_tokens'])\n",
    "\n",
    "    # Проверка формата (список списков)\n",
    "    if isinstance(data_words[0], str):\n",
    "         data_words = [ast.literal_eval(x) for x in data_words]\n",
    "\n",
    "    # Создание биграмм\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    data_ready = [bigram_mod[doc] for doc in data_words]\n",
    "\n",
    "    # Создание словаря\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "    id2word.filter_extremes(no_below=3, no_above=0.5)\n",
    "    corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "    # Поиск оптимального количества тем\n",
    "    def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "        coherence_values = []\n",
    "        model_list = []\n",
    "        for num_topics in range(start, limit, step):\n",
    "            print(f\"Обучение модели на {num_topics} тем\")\n",
    "            model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10, workers=1, per_word_topics=False)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "        return model_list, coherence_values\n",
    "\n",
    "    print(\"Поиск тем\")\n",
    "    start_t, limit_t, step_t = 3, 9, 1\n",
    "    model_list, coherence_values = compute_coherence_values(id2word, corpus, data_ready, limit=limit_t, start=start_t, step=step_t)\n",
    "\n",
    "    # Визуализация графика Coherence\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(start_t, limit_t, step_t), coherence_values, marker='o')\n",
    "    plt.xlabel(\"Количество тем\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.show()\n",
    "\n",
    "    # Выбор лучшей модели\n",
    "    best_index = np.argmax(coherence_values)\n",
    "    lda_model = model_list[best_index]\n",
    "    num_topics = range(start_t, limit_t, step_t)[best_index]\n",
    "    best_coherence = coherence_values[best_index]\n",
    "    print(f\"Выбрана модель с {num_topics} темами (coherence score: {best_coherence:.4f}).\")\n",
    "\n",
    "     # Текстовый вывод тем\n",
    "    print(\"КЛЮЧЕВЫЕ СЛОВА ПО ТЕМАМ\")\n",
    "    topics = lda_model.print_topics(num_words=10)\n",
    "    for topic in topics:\n",
    "        print(f\"Тема {topic[0]}: {topic[1]}\")\n",
    "\n",
    "    # Визуализация облака слов\n",
    "    print(\"Генерация облаков слов\")\n",
    "    cols = 2\n",
    "    rows = (num_topics // cols) + (num_topics % cols > 0)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 5 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        topic_words = dict(lda_model.show_topic(i, 30))\n",
    "        cloud = WordCloud(background_color='white', width=600, height=400, colormap='tab10').generate_from_frequencies(topic_words)\n",
    "        axes[i].imshow(cloud)\n",
    "        axes[i].set_title(f'Тема {i}', fontsize=16)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Скрываем пустые графики\n",
    "    for j in range(num_topics, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Сохранение результатов\n",
    "    def get_main_topic_df(model, corpus):\n",
    "        results = []\n",
    "        topics_per_doc = model.get_document_topics(corpus)\n",
    "        for doc_topics in topics_per_doc:\n",
    "            sorted_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "            if sorted_topics:\n",
    "                results.append(sorted_topics[0])\n",
    "            else:\n",
    "                results.append((None, 0.0))\n",
    "        return results\n",
    "\n",
    "    topic_info = get_main_topic_df(lda_model, corpus)\n",
    "    df['dominant_topic'] = [int(x[0]) if x[0] is not None else -1 for x in topic_info]\n",
    "    df['topic_confidence'] = [float(x[1]) for x in topic_info]\n",
    "\n",
    "    # Карта тем\n",
    "    topic_mapping = {i: \", \".join([w[0] for w in lda_model.show_topic(i, 5)]) for i in range(num_topics)}\n",
    "    df['topic_keywords'] = df['dominant_topic'].map(topic_mapping)\n",
    "\n",
    "    df.to_csv(TOPIC_CSV, index=False)\n",
    "    print(f\"Файл сохранен: {TOPIC_CSV}\")\n",
    "\n",
    "    # Интерактивная визуализация\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    display(vis)\n",
    "\n",
    "else:\n",
    "    print(\"Датафрейм пуст.\")"
   ],
   "id": "eeb40195d8bee5ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3667c2d0f0048f15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
